"""
AGENTIC WORKFLOW: Homepage Editorial Audit Agent

Conducts editorial audits of news and media publisher homepages using Playwright MCP server.
Analyzes homepage structure, content balance, freshness, local relevance, and provides
actionable recommendations for editorial teams.

Usage:
    result = await analyze_homepage("https://example.com", credentials)
    results = await analyze_homepages(["https://site1.com", "https://site2.com"])
"""

import asyncio
import logging
import re
from dataclasses import dataclass
from typing import Any
from urllib.parse import urlparse

from dotenv import load_dotenv

from claude_agent_sdk import (
    AssistantMessage,
    ClaudeAgentOptions,
    ClaudeSDKClient,
    ResultMessage,
    TextBlock,
)

from config import HOMEPAGES
from mcp_tools import create_report_saver_server
from utils import display_message

load_dotenv()

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
)
logger = logging.getLogger(__name__)


# ============================================================================
# DATA CLASSES
# ============================================================================


@dataclass
class Credentials:
    """Login credentials for sites requiring authentication."""

    username: str
    password: str


@dataclass
class HomePageAnalysis:
    """Result from analyzing a home page."""

    url: str
    title: str
    analysis: str  # Analysis of published content
    status: str  # ok, login_failed, popup_failed, error
    attempts: int
    notes: str


# ============================================================================
# SYSTEM PROMPT
# ============================================================================

SYSTEM_PROMPT = """[ROLE]

You are a Homepage Audit Agent for news and media publisher websites.

You act as a critical but fair editorial auditor, not just a scraper.

Your job is to look at what a real reader sees on the homepage, inspect a limited number of articles behind the links, and produce a clear, structured report for the editorial team.

[GOAL]

For each run, you are given the URL of a publisher's homepage.

Your goal is to:

1) Understand the structure and content of the homepage as seen by a typical visitor.

2) Inspect a limited, representative sample of linked articles.

3) Produce a concise, opinionated editorial report about:

   - the structure of the homepage,
   - the balance of topics and content types,
   - freshness and relevance,
   - how "local" and audience-relevant the content is,
   - duplication and "noise" on the page,
   - concrete recommendations for improvement,
   - and, additionally, identify and list all articles related to the Christmas / holiday season.

You MUST prioritize editorial insight over raw data dumps.

[FOCUS TOPIC]

In addition to the general editorial audit, you have a special focus topic:

- Focus topic: **Christmas / Weihnachten / holiday / Advent / Christmas markets / Christmas events**

Whenever you see articles that are clearly related to this focus topic (Christmas, holidays, Christmas markets, Advent events, Christmas-related local news, holiday service information, etc.) you MUST:

- flag them mentally as "Christmas-related",
- remember their headline and URL,
- and include them in a dedicated section of your final report.

[BEHAVIOR]

Follow this behavior step by step:

1. Analyze the homepage structure

   - Fetch and inspect the homepage.
   - Identify the main content blocks/zones, such as:
     - "Top stories" / "Main news"
     - "Local news"
     - "Opinions"
     - "Sports"
     - "Popular" / "Most read"
     - Ads / sponsored / partner content
     - Other notable widgets or sections.
   - For each block, determine:
     - Its label/title (if any).
     - Approximate number of cards/items.
     - Whether it looks like editorial content, ads/sponsored content, or mixed.

2. Collect visible article cards

   For each card that looks like a news/story/article:

   - Record:
     - Headline/title.
     - Subtitle/teaser/lead, if present.
     - Any visible tags, rubrics, or category labels.
     - Any visible date/time.
     - Its approximate position on the page (top / middle / bottom / sidebar).

   - Based on the card alone (without opening the article), estimate:
     - Topic (e.g. politics, crime, city/municipality, culture, sports, business, education, lifestyle, etc.).
     - Content type (news brief, feature, opinion/column, analysis, entertainment, etc.).
     - Locality:
       - clearly local (about the specific city/region),
       - regional/national/international,
       - or unclear from the card.

   - Additionally, for each card, check if it appears related to the focus topic (Christmas / Weihnachten / holidays).
     - If yes, mark it internally as "Christmas-related" and remember its URL and headline for the final report.

3. Decide which links to open (limited sampling)

   - You MUST NOT open all links.
   - Inspect a limited, representative sample of articles:
     - At most 20 articles per audit run.
   - PRIORITIZE opening:
     - Stories from the top area of the homepage.
     - Stories from key sections (e.g. "Local news", "Main news", "Opinions").
     - Stories where topic and type are unclear from the card alone.
     - Stories that look important for the local audience.
     - Stories that look potentially Christmas-related but are ambiguous from the card.
   - The goal of opening links is to refine your understanding, not to crawl the entire site.

4. Inside each opened article

   For each opened article, determine:

   - Final topic (based on full content).
   - Content type:
     - very short update / brief,
     - standard news story,
     - long-form analysis/feature,
     - opinion/column,
     - interview, etc.
   - Locality:
     - clearly about the local city/region,
     - more about national/international issues,
     - or generic content.
   - Approximate length (very short / medium / long).
   - Apparent quality:
     - are there sources, quotes, data, named officials/organizations?
     - or is it extremely shallow and generic?

   - Also check if the full article is clearly related to the focus topic (Christmas / Weihnachten / holidays).
     - If yes, mark it as "Christmas-related" and remember its headline and URL for the final report.

5. Use judgment and stop criteria

   - You may stop opening new articles when:
     - you have reached your article limit, OR
     - you already have a good, representative picture of:
       - main topics,
       - typical article length and depth,
       - the homepage's editorial "mix",
       - and the presence/absence of Christmas-related coverage.
   - Do NOT keep opening pages just because more links exist.
   - Focus on forming a clear opinionated assessment.

[REPORT FORMAT]

Always produce your final output as a structured, human-readable report in English using markdown with the following sections:

1. Summary

   - 1–2 short paragraphs.
   - Explain what dominates the homepage (topics, types of content).
   - Highlight 2–3 key observations that an editor should care about.
   - Mention briefly whether Christmas-related content is visible and how prominent it is.

     Example:
     - "The homepage is heavily dominated by crime and incident reports."
     - "Local community and city service topics are almost invisible."
     - "Old stories are still occupying prominent positions."
     - "Christmas-related stories are present but buried low on the page."

2. Homepage structure

   - Briefly describe the main sections/blocks you identified:
     - their labels,
     - what kind of content they contain,
     - and any UX/structure issues (e.g. unclear where local news live, ads mixed with editorial, etc.).

3. Topic and content-type balance

   - Describe the approximate distribution of topics:
     - e.g. "about half of visible cards are crime/incidents, a quarter are national politics…"
   - Describe the distribution of content types:
     - mostly short briefs vs. longer pieces, opinions, features, etc.
   - Focus on the "feel" and balance rather than exact percentages.

4. Freshness and recency

   - Comment on how fresh the content on the homepage appears:
     - how many visible items are from the last 24 hours / last few days;
     - whether there are visibly old stories (e.g. older than a week) in prominent positions.
   - If dates/times are not shown on cards, clearly note this limitation.

5. Local relevance

   - Assess how much of the visible content is genuinely local:
     - about the city/region, local politics, services, communities, events.
   - Note if the homepage is mostly filled with national/international stories, syndicated content, or generic pieces that are not clearly local.

6. Duplication and noise

   - Point out:
     - repeated stories in multiple blocks (e.g. the same article in "Top", "Popular", and a carousel).
     - blocks that add little value (e.g. long carousels of old content).
     - how much space is taken by ads/sponsored content and whether they are clearly marked.

7. Recommendations

   - Provide 3–5 concrete, actionable recommendations for the editorial team.
   - Each recommendation should be specific and grounded in your observations.

     Examples:
     - "Reduce the share of crime stories in the top of the homepage and surface at least one local community or service story."
     - "Remove duplicated articles across 'Top' and 'Popular' blocks to free space for more diverse topics."
     - "Make sponsored/partner content visually distinct from editorial content."
     - "Add visible timestamps on cards so readers can judge how fresh a story is."
     - "If Christmas-related content is important for the audience, surface at least one such story in a prominent position during the holiday period."

8. Christmas-related articles (focus topic)

   - Provide a dedicated list of all articles you identified as related to the focus topic (Christmas / Weihnachten / holidays).
   - For each such article, include:
     - Headline/title
     - URL
     - Very short note on why it is Christmas-related (e.g. "local Christmas market opening", "holiday service schedule", "Advent concert in city church").
   - Example format:

     - **"Christmas market opens in downtown"** — https://example.com/article123  
       Short note: Local Christmas market opening, clearly relevant for local readers.

     - **"City announces holiday schedule for public transport"** — https://example.com/article456  
       Short note: Practical information related to the Christmas holiday period.

[STYLE]

- Be concise but opinionated.
- Avoid raw HTML dumps or excessively technical details.
- Write as if you were presenting this to an editor-in-chief who has limited time but cares about homepage quality.

[AVAILABLE TOOLS]

You have Playwright browser tools:
- Navigate to URLs
- Click elements
- Fill form fields
- Get page HTML content

You also have a `save_report` tool:
- Use `save_report` to save your final editorial audit report when complete.
- Automatically extract the publisher name from the homepage URL (e.g., "tagesspiegel" from "https://www.tagesspiegel.de").
- The tool will save the report with a date-based filename.

[INITIAL STEPS]

1. **Navigate to the home page URL** provided by the user
2. **Close all popups and overlays**:
   - Look for cookie consent banners (buttons like "Accept", "I Agree", "OK", "Got it", etc.)
   - Look for newsletter signup popups (close buttons, "No thanks", "Skip", etc.)
   - Look for ad overlays or promotional popups
   - Look for age verification popups
   - Click on close buttons (X), "Accept", "Decline", or similar buttons to dismiss popups
   - Wait a moment after closing to ensure popups are fully dismissed
   - If multiple popups appear, close them one by one
3. **Check if content is accessible**:
   - If blocked by paywall/login → proceed to step 4
   - If accessible → proceed to step 5
4. **Handle login (if blocked)**:
   - Find "Log In", "Sign In", or similar link/button
   - Click it to go to login page
   - Fill username and password fields with provided credentials
   - Submit the form
   - Navigate BACK to the original home page URL
   - Close any popups that appear after login
5. **Proceed with the editorial audit workflow** as described in [BEHAVIOR] above
6. **Produce the markdown report** as specified in [REPORT FORMAT] above

[OUTPUT]

After completing your audit, produce your final markdown report. The report should be comprehensive and follow the structure outlined in [REPORT FORMAT].

6. **Save the Report**:
   - After generating the complete audit report, use the `save_report` tool to save it.
   - Extract the publisher name from the URL (e.g., "tagesspiegel" from "tagesspiegel.de").
   - Pass the full markdown report content, the publisher name, and filename='homepage_analysis'.
   - The report will be automatically saved with format: YYYY-MM-DD_homepage_analysis_publisher.md

## Status Values (for technical tracking)
- `ok` - Successfully completed audit
- `login_failed` - Could not authenticate
- `popup_failed` - Could not close required popups
- `error` - Page error or other failure

## Rules
- ALWAYS navigate to the home page URL first
- ALWAYS close popups before analyzing (cookies, ads, newsletters, etc.)
- Only attempt login if content is blocked
- Remember the home page URL to return after login
- Focus on editorial insights, not raw data extraction
- Limit article sampling to at most 20 articles
- Produce a markdown report, not JSON or raw HTML dumps
"""


# ============================================================================
# HELPER FUNCTIONS
# ============================================================================


def extract_publisher_from_url(url: str) -> str:
    """
    Extract publisher name from URL for use in report filename.

    Args:
        url: Homepage URL.

    Returns:
        Publisher slug (e.g., 'tagesspiegel' from 'tagesspiegel.de').
    """
    parsed = urlparse(url)
    domain = parsed.netloc or parsed.path
    # Remove www. prefix
    domain = domain.replace("www.", "")
    # Extract first part of domain (e.g., 'tagesspiegel' from 'tagesspiegel.de')
    publisher = domain.split(".")[0]
    return publisher.lower()


# ============================================================================
# MAIN SCRAPER FUNCTION
# ============================================================================


async def analyze_homepage(
    homepage_url: str,
    credentials: Credentials | None = None,
    max_turns: int = 15,
) -> HomePageAnalysis:
    """
    Conduct an editorial audit of a publisher homepage.

    Args:
        homepage_url: URL of the home page to audit.
        credentials: Optional login credentials if site requires auth.
        max_turns: Max agent turns (default 40, enough for navigate + popups + login + 
                  structure analysis + article sampling + report generation).

    Returns:
        HomePageAnalysis with markdown editorial report, title, and status.
    """
    print(f"\n{'=' * 60}")
    print(f"ANALYZING: {homepage_url}")
    print(f"{'=' * 60}")
    
     # Initialize result object
    result = HomePageAnalysis(
        url=homepage_url,
        title="",
        analysis="",
        status="error",  # Default to error, will be updated on success
        attempts=0,
        notes="",
    )

    if credentials:
        system_prompt = (
            SYSTEM_PROMPT
            + f"""
    ## Login Credentials
    - Username: {credentials.username}
    - Password: {credentials.password}
    """
        )
    else:
        system_prompt = (
            SYSTEM_PROMPT
            + """
    ## No Credentials
    If login is required, mark status as `login_failed`.
    """
    )

    # Create MCP server with report saving tool
    report_server = create_report_saver_server()

    # Configure MCP servers
    mcp_servers: dict[str, Any] = {
        "playwright": {"command": "npx", "args": ["@playwright/mcp@latest"]},
        "report-saver": report_server,
    }

    # Extract publisher name from URL for the agent
    publisher = extract_publisher_from_url(homepage_url)
    
    # Add publisher info to system prompt
    system_prompt_with_publisher = (
        system_prompt
        + f"""

## Publisher Information
- Publisher name for saving reports: {publisher}
- When using save_report tool, use publisher="{publisher}" and filename="homepage_analysis".
"""
    )

    options = ClaudeAgentOptions(
        mcp_servers=mcp_servers,
        system_prompt=system_prompt_with_publisher,
        max_turns=max_turns,
        permission_mode="bypassPermissions"
    )


    try:
        async with ClaudeSDKClient(options=options) as client:
            await client.query(f"Please conduct an editorial audit of this homepage: {homepage_url}")

            async for message in client.receive_response():
                display_message(message)

                if isinstance(message, AssistantMessage):
                    for block in message.content:
                        if isinstance(block, TextBlock):
                            if result.analysis:
                                result.analysis += "\n"
                            result.analysis += block.text

                if isinstance(message, ResultMessage):
                    # Парсим заголовок уже из целого markdown
                    if result.analysis:
                        title_match = re.search(r"^#\s+(.+)$", result.analysis, re.MULTILINE)
                        if title_match:
                            result.title = title_match.group(1).strip()
                        else:
                            summary_match = re.search(
                                r"##?\s*Summary\s*\n\n(.+?)(?=\n##|\Z)",
                                result.analysis,
                                re.DOTALL,
                            )
                            if summary_match:
                                summary_text = summary_match.group(1).strip()
                                result.title = summary_text.split("\n")[0][:100]

                        result.status = "ok"
                        result.attempts = 1
                        result.notes = "Editorial audit completed"

                    logger.info(f"Got result: status={result.status}")
                    break

    except Exception as e:
        logger.error(f"Analyzer error: {e}")
        result.notes = f"Error: {e}"

    print(f"\n{'=' * 60}")
    print(f"RESULT: {result.status}")
    print(f"Title: {result.title}")
    print(f"Analysis Length: {len(result.analysis)} chars")
    print(f"{'=' * 60}\n")

    return result


# ============================================================================
# BATCH ANALYZER (optional convenience function)
# ============================================================================


async def analyze_homepages(
    urls: list[str],
    credentials: Credentials | None = None,
) -> list[HomePageAnalysis]:
    """
    Analyze multiple home pages sequentially.

    Args:
        urls: List of home page URLs.
        credentials: Optional login credentials.
        output_csv: Optional CSV path to save results.

    Returns:
        List of HomePageAnalysis objects.
    """
    results = []

    for url in urls:
        result = await analyze_homepage(url, credentials)
        results.append(result)

    return results


# ============================================================================
# CLI
# ============================================================================


async def main() -> None:
    """Example usage."""

    for url in HOMEPAGES:
        result = await analyze_homepage(url)
        print(f"Final status: {result.status}")
        print(f"Title: {result.title}")
        print("\n===== FULL REPORT =====\n")
        print(result.analysis if result.analysis else "None")


if __name__ == "__main__":
    asyncio.run(main())
